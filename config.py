from make_datasets.config import MAX_CONTEXT_NUMBER

TASK = 'GENERATION'
LOGGING_DIR = 'logs'
OUTPUT_DIR = 'results'

WARMUP_STEPS = 200
WEIGHT_DECAY = 0.1
LEARNING_RATE = 5e-5

DEFINITION_SOURCE = 'wikidata'

# ========================== GENERATION CONFIGS ==========================
MASK_ENTITY = False
MODEL_NAME = 'CME' if MASK_ENTITY else 'CPE'
TRAIN_GENERATION_BATCH_SIZE = 16
EVAL_GENERATION_BATCH_SIZE = 16
TEST_GENERATION_BATCH_SIZE = 16

INPUT_GENERATION_MAX_LENGTH = 300
OUTPUT_GENERATION_MAX_LENGTH = 20
OUTPUT_GENERATION_MIN_LENGTH = 3

MODEL_GENERATION_NAME = 'facebook/bart-large-cnn'

DATA_GENERATION_FOLDER = 'data/wikipedia/'
TRAIN_GENERATION_FILE = f'{DATA_GENERATION_FOLDER}{MAX_CONTEXT_NUMBER}_contexts_{DEFINITION_SOURCE}_train.csv'
TEST_GENERATION_FILE = f'{DATA_GENERATION_FOLDER}{MAX_CONTEXT_NUMBER}_contexts_{DEFINITION_SOURCE}_test.csv'
VALID_GENERATION_FILE = f'{DATA_GENERATION_FOLDER}{MAX_CONTEXT_NUMBER}_contexts_{DEFINITION_SOURCE}_val.csv'
# DATA_GENERATION_CATEGORY = 'human'
# DATA_GENERATION_STYLE = 'ne_with_context'
#
# TRAIN_GENERATION_FILE = f'{DATA_GENERATION_FOLDER}/train_{DATA_GENERATION_CATEGORY}_{DATA_GENERATION_STYLE}.csv'
# TEST_GENERATION_FILE = f'{DATA_GENERATION_FOLDER}/test_{DATA_GENERATION_CATEGORY}_{DATA_GENERATION_STYLE}.csv'
# VALID_GENERATION_FILE = f'{DATA_GENERATION_FOLDER}/valid_{DATA_GENERATION_CATEGORY}_{DATA_GENERATION_STYLE}.csv'

ADDITIONAL_SPECIAL_TOKENS = ['<NE>', '</NE>']

LOAD_GENERATION_MODEL = False
MODEL_GENERATION_PATH = f'results/{MAX_CONTEXT_NUMBER}_contexts_{DEFINITION_SOURCE}_{MODEL_NAME}/'

EVALUATE_GENERATION = True
PRED_GENERATION_FILE_PATH = f'{MAX_CONTEXT_NUMBER}_contexts_{DEFINITION_SOURCE}_{MODEL_NAME}_preds.csv'

MASKING_PROBABILITY = 1.0

EPOCHS = 4
# ========================== GENERATION CONFIGS ==========================

# ======================== CLASSIFICATION CONFIGS ========================
TRAIN_CLASSIFICATION_BATCH_SIZE = 128
EVAL_CLASSIFICATION_BATCH_SIZE = 128
TEST_CLASSIFICATION_BATCH_SIZE = 128

INPUT_CLASSIFICATION_MAX_LENGTH = 350

MODEL_CLASSIFICATION_NAME = 'bert-large-cased'

CLASSIFICATION_SPECIAL_TOKENS = ['<NE>', '</NE>', '[SEC]']

DATA_CLASSIFICATION_FOLDER = 'data/wikipedia/'
TRAIN_CLASSIFICATION_FILE = f'{DATA_CLASSIFICATION_FOLDER}{MAX_CONTEXT_NUMBER}_contexts_{DEFINITION_SOURCE}_classification_train.csv'
TEST_CLASSIFICATION_FILE = f'{DATA_CLASSIFICATION_FOLDER}{MAX_CONTEXT_NUMBER}_contexts_{DEFINITION_SOURCE}_classification_test.csv'
VALID_CLASSIFICATION_FILE = f'{DATA_CLASSIFICATION_FOLDER}{MAX_CONTEXT_NUMBER}_contexts_{DEFINITION_SOURCE}_classification_val.csv'
# DATA_CLASSIFICATION_CATEGORY = 'human'
# TRAIN_CLASSIFICATION_FILE = f'{DATA_CLASSIFICATION_FOLDER}/train_{DATA_CLASSIFICATION_CATEGORY}_classification.csv'
# TEST_CLASSIFICATION_FILE = f'{DATA_CLASSIFICATION_FOLDER}/test_{DATA_CLASSIFICATION_CATEGORY}_classification.csv'
# VALID_CLASSIFICATION_FILE = f'{DATA_CLASSIFICATION_FOLDER}/valid_{DATA_CLASSIFICATION_CATEGORY}_classification.csv'

LOAD_CLASSIFICATION_MODEL = False
MODEL_CLASSIFICATION_PATH = f'results/{MAX_CONTEXT_NUMBER}_contexts_{DEFINITION_SOURCE}_classifier/'

EVALUATE_CLASSIFICATION = True
PRED_CLASSIFICATION_FILE_PATH = f'{MAX_CONTEXT_NUMBER}_contexts_{DEFINITION_SOURCE}_classifier_preds.csv'
# ======================== CLASSIFICATION CONFIGS ========================
